---
layout: post
title:  "Efficient Estimation of Word Representations in Vector Space"
date:   2018-01-04 11:08:59
author: Sungwon Lyu
categories: NLP
---

## WHY? 
Continuous Word Representation을 평가할 수 있는 방법을 제시하고
더 많은 데이터들로 부터 더 빠르게 더 나은 Continuous Word Representation을 할 수 있는 Continous Bag-of-Words와 Continous Skip-gram Model을 제시

## WHAT?
NNLM : 앞 단어들로 뒷단어를 예측하도록 학습
CBOW : 앞 뒤의 단어들 각각에 대하여 중심 단어를 예측하도록 학습
Skip-gram : 중심 단어들로 앞 뒤의 단어들을 예측하도록 학습
실험: 여러 의미 관계의 Biggest - big + small 과 같은 문제를 얼마나 잘 학습했는지 확인

## So
기존 방법들보다 훨씬 빠를 뿐만이 아니라 성능도 훨씬 좋았다. Skip-gram이 semantic의미를 가장 잘 확인했고 CBOW는 syntatic의미 파악에 정확도가 높았다. 