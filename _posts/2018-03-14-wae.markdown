---
layout: post
title:  "Wasserstein Auto-Encoders"
date:   2018-03-12 19:40:59
author: Sungwon Lyu
categories: DL
---
## WHY? 
기존의 VAE에서는 reconstruction loss와 Q(z)와 posterier분포 P(z|x)간의 KL Divergence를 최소화하도록 하여 encoder와 decoder를 학습시켰다. 

## WHAT?
분포 간의 차이를 구할 때 기존의 KL Divergence보다 약화된 거리 개념인 Wasserstein distance를 사용하였다. Wasserstein distance란 다음과 같이 정의될 수 있다.\\
$$W(\mathbb{P}_{r}, \mathbb{P}_{g}) = inf_{\gamma \in \Pi(\mathbb{P}_{r}, \mathbb{P}_{g})} \mathbb{E}_{(x,y)~\gamma} [\|x - y\|]$$\\
이는 목적 분포와 우리가 만드려는 분포의 결합확률 분포의 차이의 기대값의 infimum(Greatest Lowerbound: 최대값)이다. 그러므로 두 분포간의 같은 support가 아니더라도 정의될 수 있다. 이를 Kantorovich-Rubinstein duality를 사용하여 다음과 같이 바꾼다. \\
$$W(\mathbb{P}_{r}, \mathbb{P}_{g}) = sup_{\|f\|_{L}\leq1} \mathbb{E}_{x~\mathbb{P}_{r}}[f(x)] - \mathbb{E}_{x~\mathbb{P}_{\theta}}[f(x)]$$\\
이리하여 WGAN은 두 분포의 Wasserstein distance를 구하기 위하여 위 식을 최대화하는 단계와 도출한 식을 최소화 하도록 generator를 학습시키는 두 단계를 거친다. 알고리즘은 다음과 같다. 
![img](/assets/images/wgan.png){: .center-image width="50px"}

## So?
WGAN은 두 가지 강점을 가진다. 두 분포간의 Wasserstein distance와 생성된 표본의 품질이 비례하여 하나의 메트릭으로 참고할 수 있다. 또한 GAN보다 안정적으로 학습할 수 있다. 
