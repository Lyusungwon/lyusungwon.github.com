---
layout: post
title:  "Model-Free Episodic Control"
date:   2018-03-26 16:08:59
author: Sungwon Lyu
categories: RL
---

## WHY? 
인간은 어떤 환경에서 한번 보상을 보게 되면 그에 대해 빠르게 학습한다. 
[DQN](https://lyusungwon.github.io/rl/2018/01/25/dqn.html)은 환경을 학습하기 위하여 그 상태와 행동의 가치를 근사하지만 환경 전체를 알기 위해서는 아주 오래걸린다. 

## WHAT?
Model-free Episodic Control에서는 한번 보상을 얻으면 그 경험을 적극적으로 활용한다. 이는 환경이 deterministic하고 exploration보다 exploitation이 더 중요한 미로와 같은 환경에서 적합하다.\\
한 에피소드가 끝나고 보상을 얻고나서 그 에피소드에서 거쳤던 모든 상태와 행동의 값인 $$Q^{EC}(s_t, a_t)$$를 discount된 보상값으로 업데이트 한다. 만약 기존 값이 있다면 기존 값과 새 값을 비교하여 큰 값을 넣는다. 이러한 업데이트 방법에서는 모든 상태와 행동을 모두 거쳐야 하기 때문에 공백이 많아질 수 있다. 그렇기 때문에 각 스텝에서 행동을 결정할 때 그 값이 존재하지 않는다면 그 상태와 가장 유사한 k개의 값들을 평균해서 사용한다. 이를 $$\hat{Q^{EC}}(s, a)$$로 표현한다. 상태의 차원이 높다면 유사한 차원을 찾는데 시간이 오래 걸리므로 차원을 축소하는 random matrix를 곱하거나 vae를 사용하여 상태를 축소한다. 알고리즘은 다음과 같다. \\
![image](/assets/images/mfec.png){: .center-image width="50px"}

## So?
이를 통하여 deterministic한 환경에서 아주 빠르고 효율적으로 학습하여 좋은 성과를 거두었다. 

[Blundell, Charles, et al. "Model-free episodic control." arXiv preprint arXiv:1606.04460 (2016).
](https://arxiv.org/abs/1606.04460)