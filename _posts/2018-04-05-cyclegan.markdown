---
layout: post
title:  "Unpaired Image-to-image Translation using Cycle-Consistent Adversarial Network"
date:   2018-04-05 11:24:59
author: Sungwon Lyu
categories: studies
tags: deep-learning generative-models
---
## WHY? 
이미지들 간의 style을 뽑아내기 위해서는 같은 모습에 대하여 다른 스타일이 적용되어 있는 이미지들이 pairing되어야 한다. 

## WHAT?
이를 위하여 두 이미지의 스타일을 서로 줄 수 있는 CycleGAN을 제안한다. CycleGAN은 X->Y, Y->X로 변환하는 두가지 encoder겸 decoder로 이루어져 있다. CycleGAN의 Loss는 두 부분으로 이루어진다. 첫번째는 Adversarial Loss인데 y로 변한 x의 adversarial loss와 x로 변한 y의 adversarial loss을 모두 계산한다. 두번째는 cycle consistency loss로 두번의 변환 결과 원래 모습 그대로 돌아오도록 강제한다. 
![image](/assets/images/cyclegan.png){: .center-image width="50px"}

## So?
Unsupervise한 방법으로 style transfer를 쉽게 할 수 있게 되었다. 

[Zhu, Jun-Yan, et al. "Unpaired image-to-image translation using cycle-consistent adversarial networks." arXiv preprint arXiv:1703.10593 (2017).](https://arxiv.org/abs/1703.10593)
