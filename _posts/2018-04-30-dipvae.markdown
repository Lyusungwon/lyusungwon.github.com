---
layout: post
title:  "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations"
date:   2018-04-30 11:46:59
author: Sungwon Lyu
categories: DL
---

## WHY? 
Disentanling과정은 기본적으로 x내에서 독립적인 요소를 찾아 각각 다른 z로 나누는 작업이다. 이를 위하여 z의 prior를 independent Gaussian(N(1,0))로 간주하여 근사하거나([Beta-VAE]()) Batch내의 z의 값을 permutation하여 adversarial training하는 방법으로 독립을 유도하였다([FVAE]()). 하지만 Beta-VAE는 모든 관측치의 분포를 N(1,0)으로 강제하여 관측치의 차이에 덜 민감하게 만들어 reconstruction의 성능이 떨어진다.  

## WHAT?
DIP-VAE(Disentangled Inferred Prior VAE)는 encoder의 결과를 독립한 prior 분포에 거리로서 근사하는 것이 아니라 두 분포의 Covariance를 맞춤으로써 근사하였다. z의 Covariance는 첫번째 식과 같이 random variable을 포함하는 두 항으로 나눌 수 있다. 기존의 VAE들은 분산과 평균을 neural net으로 근사한 정규분포로 사용하였기 때문에 두번째 식을 Identity matrix에 근사함으로써 독립 분포에 근사할 수 있다. 
$$Cov_{q_{\phi}(z)}[z] = E_{p_{\phi}(x)}Cov_{q_{\phi}(z|x)}[z] + Cov_{p(x)}(E_{q_{\phi}(z|x)[z])\\
Cov_{q_{\phi}(z)}[z] = E_{p(x)}[\Sigma_{\phi}(x)] + Cov_{p(x)}[\mu_{\phi}(x)]$$
일반적으로 $$\Sigma_{\phi}(x)$$는 Diagonal matrix로 취해지기 때문에 z간의 correlation은 $$Cov_{p(x)[\mu_{\phi}(x)]}$$로 부터만 발생한다. 이로 인하여 두 가지 모델이 생길 수 있는데 $$Cov_{p(x)}[\mu_{\phi}(x)]$$를 조절하는 DIP-VAE-I과 $$Cov_{q_{\phi}}[z]$$를 조절하는 DIP-VAE-II가 그것이다. 각 Covariance의 Diagonal term을 1로 제한하고 나머지를 0으로 제한함으로써 각각 다음과 같은 목적식을 가지게 된다. 
$$max_{\theta, \phi}ELBO(\theta, \phi) - \lambda_{od}\Sigma_{i\neq j}[Cov_{p(x)}[\mu_{\phi}(x)]]^2_{ij} - \lambda_{d}\Sigma_{i}([Cov_{p(x)}[\mu_{\phi}(x)]]_{ii})^2\\
max_{\theta, \phi}ELBO(\theta, \phi) - \lambda_{od}\Sigma_{i\neq j}[Cov_{q_{\phi}}[z]]^2_{ij} - \lambda_{d}\Sigma_{i}([Cov_{q_{\phi}}[z]]_{ii})^2$$
또한 Disentanglement를 측정하는 방법으로 SAP Score(Separated Attribute Predictability score)를 제안하였다. 

## So?
DIP-VAE는 Beta-VAE와는 달리 Reconstruction 성능을 희생하지 않고도 disentanglement를 해낼 수 있었으며 SAP와 Z-diff에서 대체로 나은 성과를 보였다. 또한 Z-diff에 비하여 SAP가 정성적 실험결과와 부합하는 것을 확인하였다. 

## Critic
수학적으로 뒷받침이 잘 되어서 좋았다. 하지만 내가 수학이 부족해서 그런지 다소 난해하게 느껴졌다. 

[Kumar, Abhishek, Prasanna Sattigeri, and Avinash Balakrishnan. "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations." arXiv preprint arXiv:1711.00848 (2017).](https://arxiv.org/abs/1711.00848)
