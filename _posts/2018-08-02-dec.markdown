---
layout: post
title:  "Unsupervised Deep Embedding for Clustering Analysis"
date:   2018-08-02 13:41:59
author: Sungwon Lyu
categories: Natural-Language-Processing
---

## WHY? 
There had been little study on learning representation that focus on clustering.

## WHAT?
Deep Embedding Clustering(DEC) consists of two phases: parameter initialization with a deep autodncoder and (2) parameter optimization. This paper first describe the send 


$$
q_{ij} = \frac{(1+\|z_i - \mu_j\|^2/\alpha)^{-\frac{\alpha+1}{2}}}{\sum_{j'}(1+\|z_i - \mu_j\|^2/\alpha)^{-\frac{\alpha+1}{2}}}
$$
![image](/assets/images/tsd.png){: .center-image width="50px"}

## So?
This model showed to clearly disentangle the distribution of unlabeled data while the model of [Disentangling Factors of Variation in Deep Representations Using Adversarial Training](https://lyusungwon.github.io/dl/2018/04/12/dfat.html) showed some entanglement in Z due to its Gaussian distribution assumption. This model showed clean disentanglement in MNIST, Sprites and NORB. Interestingly, this paper used the model to financial data to disentangle the information of market from the price of a specific stock. 

## Critic
Simple but clean idea to disentangle the labeled information.

[Hadad, Naama, Lior Wolf, and Moni Shahar. "A Two-Step Disentanglement Method." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0473.pdf)