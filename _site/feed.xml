<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sungwon's blog</title>
    <description>Personal blog with some machine learning.
</description>
    <link>http://lyusungwon.github.io/</link>
    <atom:link href="http://lyusungwon.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 02 Feb 2018 22:09:38 +0900</pubDate>
    <lastBuildDate>Fri, 02 Feb 2018 22:09:38 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Human-level control through deep reinforcement learning</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;기존의 강화학습 agent들은 각 게임마다 다른 feature를 추출하여 state로 사용하였기 때문에 각 게임마다 다른 모델로 학습해야 한다는 한계가 있었다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;Deep convolution network 를 사용하여 게임 화면의 픽셀들로 부터 피쳐들을 학습하여 state로 사용하는 모델을 제시하였다. 샘플간의 공분산성을 해결하기 위하여 replay memory method를  사용하였고 moving target현상을 해결하기 위하여 메인 모델과 target모델을 다르게 유지하는 기법을 사용하였다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;한 모델을 통하여 많은 종류의 아타리 게임들에서 기존의 기법이나 사람보다 나은 결과를 가져왔다.&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jan 2018 21:21:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/rl/2018/01/25/8.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/rl/2018/01/25/8.html</guid>
        
        
        <category>RL</category>
        
      </item>
    
      <item>
        <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;기존의 encoder-decoder model들은 인코더의 맨 마지막 벡터의 모든 input정보가 담겨야 해서 긴 문장을 번역하는데 한계가 있었다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;맨 마지막 벡터 대신&lt;br /&gt;
input벡터들의 hidden layer와 decoder의 이전 hidden layer와의 곱을 점수로 매겨&lt;br /&gt;
그 점수로 input벡터들의 hidden layer들을 가중합한 context 벡터를 활용하여&lt;br /&gt;
decoder의 다음 hidden layer를 구한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;번역을 할 때 input 문장을 통째로 외워서 번역하는 것이 아니라 각 단어에 해당하는 단어를 참고하면서 번역하는 효과를 가져와 훨씬 효과가 좋았다.&lt;/p&gt;

&lt;p&gt;참고: https://github.com/Lyusungwon/nmt&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Jan 2018 05:58:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/23/7.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/23/7.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Dynamic Topic Model</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;LDA를 통하여 토픽 모델링을 할 때 시간적으로 토픽과 그에 대한 단어 분포가 변화하는 정보를 반영하지 못한다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/dynamic_topic_model.png&quot; alt=&quot;image&quot; class=&quot;center-image&quot; width=&quot;50px&quot; /&gt;
기존의 LDA의 모델에서 파라미터(토픽과 토픽의 비율 - &lt;script type=&quot;math/tex&quot;&gt;\alpha, \beta&lt;/script&gt;)를 평균으로 정규분포를 통하여 표본 추출했다고 가정함으로서 시계열적인 latent variable을 가지고 있다고 가정하고 근사 추정을 한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;LDA보다 시계열적인 정보를 잘 반영한다.&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Jan 2018 05:38:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/pgm/2018/01/23/6.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/pgm/2018/01/23/6.html</guid>
        
        
        <category>PGM</category>
        
      </item>
    
      <item>
        <title>GloVe: Global Vectors for Word Representation</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;기존의 Skipgram과 CBOW는 일정 window 내의 정보만 반영할 뿐 global한 frequency정보는 반영하지 못한다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{J} = \Sigma_{i, j}f(X_{i, j})(w_{i}^{T}\tilde{w}-logX_{ij})^{2}&lt;/script&gt;&lt;br /&gt;
두 임베딩의 곱을 동시발생 빈도만큼 가중하여 두 단어의 동시발생 빈도에 가까워 지도록 학습하는 GloVe를 제안하였다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;Skipgram과 CBOW보다 여러면에서 좋은 성과를 내었다.&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Jan 2018 06:48:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/08/5.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/08/5.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>node2vec</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;네트워크의 노드들을 Skipgram과 같이 distributed representation을 통하여 나타내려고 했다. 최근의 이러한 시도들은 노드의 이웃들을 경직되게 정의하여 다양한 네트워크 모양의 패턴을 파악하는데 실패하였다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;node를 벡터화하는데 node의 이웃을 유연하게 정의한 node2vec을 고안하였다. Random walk 방법을 통하여 노드의 이웃을 샘플링하는데 다시 돌아올 확률(1/p), 이 전 노드와 이어진 노드로 이동할 확률(1), 그리고 그렇지 않은 확률(1/q)을 하이퍼파라미터로 설정하였다. p가 작다면 node2vec은 균일성(homogenity)를 학습하게 되고 q가 작다면 노드의 구조적 특성을 학습하게 된다. 두 노드의 벡터를 통하여 엣지를 나타내는 표현형을 찾아본 결과 Hadamard Product가 가장 효과적이었다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;다양한 p, q를 통하여 학습한 결과 multilabel classification에서 다른 모델보다 좋은 성능을 보였고 Link prediction에서도 좋은 성능을 보였다. Node2vec은 대부분의 파라미터에 대하여 우수하고 노이즈에 강하며 Scalable하다.&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 22:49:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/pgm/2018/01/06/4.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/pgm/2018/01/06/4.html</guid>
        
        
        <category>PGM</category>
        
      </item>
    
      <item>
        <title>Distributed Representations of Words and Phrases and their Compositionality</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;최근 제시된 Skipgram의 성능을 향상시킬만한 여러 기법을 시행하였다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;Subsampling of the frequent words: 너무 자주 등장하는 단어들은 임베딩에 거의 영향을 주지 않음으로 일정 Threshold이상의 단어들은 제외한다.&lt;br /&gt;
Negative Sampling: 단어 벡터가 클 경우 Cross-entropy의 계산량이 많으므로 Hierarchical softmax보다 좋은 Negative sampleing을 사용한다. Noise distribution &lt;script type=&quot;math/tex&quot;&gt;\frac{U^{\frac{3}{4}}}{Z}&lt;/script&gt;를 통하여 k개의 Negative sample을 추출하여 loss에 반영한다. &lt;br /&gt;
Phrase vector: 하나 이상의 단어로 이루어진 구의 임베딩을 위하여 일정 스코어를 통하여 타당한 구를 추출하고 그 구를 하나의 단어처럼 학습한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;Subsampling과 Negative Sampling을 통하여 더 빠르면서 우수한 성능을 보였다.&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 00:35:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/05/3.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/05/3.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Distributed Representations of Sentences and Documents</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;한 단어의 길이는 정해져 있지만 한 문단이나 문서의 길이는 정해져 있지 않기 때문에 하나의 벡터로 만드는데 어려움이 있다. 이를 극복하기 위해 Paragraph Vector를 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;먼저 앞 단어들의 Concat나 평균으로 다음 단어를 예측하는 단어 행렬 W을 학습한다.&lt;br /&gt;
A distributed memory model: 위와 같은 예측을 하면서 맨 앞에 문단 행렬 D를 추가하여 학습한다. 이때 문단에서 일정 길이의 context를 sample하며 D를 학습한다. &lt;br /&gt;
A distributed bag of words: 문단에서 일정 길이의 context를 sample하여 문단 행렬 D로 이 문단의 단어들을 예측하는 학습을 한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;Sentimental Analysis와 Information Retrieval 평가에서 Paragraph Vector가 압도적인 성능을 보였다. PV-DM이 PV-DBOW보다 더 나은 성능을 보이나 둘이 섞을 경우 최고의 성능을 보였다.&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 20:59:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/04/2.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/04/2.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Efficient Estimation of Word Representations in Vector Space</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;Continuous Word Representation을 평가할 수 있는 방법을 제시하고&lt;br /&gt;
더 많은 데이터들로 부터 더 빠르게 더 나은 표현을 학습할 수 있는 Continous Bag-of-Words(CBOW)와 Continous Skip-gram Model을 제시&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;NNLM : 앞 단어들로 뒷단어를 예측하도록 학습&lt;br /&gt;
CBOW : 앞 뒤의 단어들 각각에 대하여 중심 단어를 예측하도록 학습&lt;br /&gt;
Skip-gram : 중심 단어들로 앞 뒤의 단어들을 예측하도록 학습&lt;br /&gt;
실험: 여러 의미 관계의 Biggest - big + small 과 같은 문제를 얼마나 잘 학습했는지 확인&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;기존 방법들보다 훨씬 빠를 뿐만이 아니라 성능도 훨씬 좋았다. Skip-gram이 semantic의미를 가장 잘 확인했고 CBOW는 syntatic의미 파악에 정확도가 높았다.&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 20:08:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/04/1.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/04/1.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
  </channel>
</rss>
