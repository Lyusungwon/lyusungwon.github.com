<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sungwon's blog</title>
    <description>Personal blog with some machine learning.
</description>
    <link>http://lyusungwon.github.io/</link>
    <atom:link href="http://lyusungwon.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 03 Feb 2018 01:05:36 +0900</pubDate>
    <lastBuildDate>Sat, 03 Feb 2018 01:05:36 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Human-level control through deep reinforcement learning</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;기존의 강화학습 agent들은 각 게임마다 다른 feature를 추출하여 state로 사용하였기 때문에 각 게임마다 다른 모델로 학습해야 한다는 한계가 있었다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;Deep convolution network 를 사용하여 게임 화면의 픽셀들로 부터 피쳐들을 학습하여 state로 사용하는 모델을 제시하였다. 샘플간의 공분산성을 해결하기 위하여 replay memory method를  사용하였고 moving target현상을 해결하기 위하여 메인 모델과 target모델을 다르게 유지하는 기법을 사용하였다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;한 모델을 통하여 많은 종류의 아타리 게임들에서 기존의 기법이나 사람보다 나은 결과를 가져왔다.&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jan 2018 21:21:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/rl/2018/01/25/8.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/rl/2018/01/25/8.html</guid>
        
        
        <category>RL</category>
        
      </item>
    
      <item>
        <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;기존의 encoder-decoder model들은 인코더의 맨 마지막 벡터의 모든 input정보가 담겨야 해서 긴 문장을 번역하는데 한계가 있었다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;맨 마지막 벡터 대신&lt;br /&gt;
input벡터들의 hidden layer와 decoder의 이전 hidden layer와의 곱을 점수로 매겨&lt;br /&gt;
그 점수로 input벡터들의 hidden layer들을 가중합한 context 벡터를 활용하여&lt;br /&gt;
decoder의 다음 hidden layer를 구한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;번역을 할 때 input 문장을 통째로 외워서 번역하는 것이 아니라 각 단어에 해당하는 단어를 참고하면서 번역하는 효과를 가져와 훨씬 효과가 좋았다.&lt;/p&gt;

&lt;p&gt;참고: https://github.com/Lyusungwon/nmt&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Jan 2018 05:58:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/23/7.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/23/7.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Dynamic Topic Model</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;LDA를 통하여 토픽 모델링을 할 때 시간적으로 토픽과 그에 대한 단어 분포가 변화하는 정보를 반영하지 못한다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/dynamic_topic_model.png&quot; alt=&quot;image&quot; class=&quot;center-image&quot; width=&quot;50px&quot; /&gt;
기존의 LDA의 모델에서 파라미터(토픽과 토픽의 비율 - &lt;script type=&quot;math/tex&quot;&gt;\alpha, \beta&lt;/script&gt;)를 평균으로 정규분포를 통하여 표본 추출했다고 가정함으로서 시계열적인 latent variable을 가지고 있다고 가정하고 근사 추정을 한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;LDA보다 시계열적인 정보를 잘 반영한다.&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Jan 2018 05:38:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/pgm/2018/01/23/6.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/pgm/2018/01/23/6.html</guid>
        
        
        <category>PGM</category>
        
      </item>
    
      <item>
        <title>GloVe: Global Vectors for Word Representation</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;기존의 Skipgram과 CBOW는 일정 window 내의 정보만 반영할 뿐 global한 frequency정보는 반영하지 못한다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{J} = \Sigma_{i, j}f(X_{i, j})(w_{i}^{T}\tilde{w}-logX_{ij})^{2}&lt;/script&gt;&lt;br /&gt;
두 임베딩의 곱을 동시발생 빈도만큼 가중하여 두 단어의 동시발생 빈도에 가까워 지도록 학습하는 GloVe를 제안하였다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;Skipgram과 CBOW보다 여러면에서 좋은 성과를 내었다.&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Jan 2018 06:48:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/08/5.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/08/5.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>node2vec</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;네트워크의 노드들을 Skipgram과 같이 distributed representation을 통하여 나타내려고 했다. 최근의 이러한 시도들은 노드의 이웃들을 경직되게 정의하여 다양한 네트워크 모양의 패턴을 파악하는데 실패하였다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;node를 벡터화하는데 node의 이웃을 유연하게 정의한 node2vec을 고안하였다. Random walk 방법을 통하여 노드의 이웃을 샘플링하는데 다시 돌아올 확률(1/p), 이 전 노드와 이어진 노드로 이동할 확률(1), 그리고 그렇지 않은 확률(1/q)을 하이퍼파라미터로 설정하였다. p가 작다면 node2vec은 균일성(homogenity)를 학습하게 되고 q가 작다면 노드의 구조적 특성을 학습하게 된다. 두 노드의 벡터를 통하여 엣지를 나타내는 표현형을 찾아본 결과 Hadamard Product가 가장 효과적이었다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;다양한 p, q를 통하여 학습한 결과 multilabel classification에서 다른 모델보다 좋은 성능을 보였고 Link prediction에서도 좋은 성능을 보였다. Node2vec은 대부분의 파라미터에 대하여 우수하고 노이즈에 강하며 Scalable하다.&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 22:49:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/pgm/2018/01/06/4.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/pgm/2018/01/06/4.html</guid>
        
        
        <category>PGM</category>
        
      </item>
    
      <item>
        <title>Distributed Representations of Words and Phrases and their Compositionality</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;최근 제시된 Skipgram의 성능을 향상시킬만한 여러 기법을 시행하였다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;Subsampling of the frequent words: 너무 자주 등장하는 단어들은 임베딩에 거의 영향을 주지 않음으로 일정 Threshold이상의 단어들은 제외한다.&lt;br /&gt;
Negative Sampling: 단어 벡터가 클 경우 Cross-entropy의 계산량이 많으므로 Hierarchical softmax보다 좋은 Negative sampleing을 사용한다. Noise distribution &lt;script type=&quot;math/tex&quot;&gt;\frac{U^{\frac{3}{4}}}{Z}&lt;/script&gt;를 통하여 k개의 Negative sample을 추출하여 loss에 반영한다. &lt;br /&gt;
Phrase vector: 하나 이상의 단어로 이루어진 구의 임베딩을 위하여 일정 스코어를 통하여 타당한 구를 추출하고 그 구를 하나의 단어처럼 학습한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;Subsampling과 Negative Sampling을 통하여 더 빠르면서 우수한 성능을 보였다.&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 00:35:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/05/3.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/05/3.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Distributed Representations of Sentences and Documents</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;한 단어의 길이는 정해져 있지만 한 문단이나 문서의 길이는 정해져 있지 않기 때문에 하나의 벡터로 만드는데 어려움이 있다. 이를 극복하기 위해 Paragraph Vector를 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;먼저 앞 단어들의 Concat나 평균으로 다음 단어를 예측하는 단어 행렬 W을 학습한다.&lt;br /&gt;
A distributed memory model: 위와 같은 예측을 하면서 맨 앞에 문단 행렬 D를 추가하여 학습한다. 이때 문단에서 일정 길이의 context를 sample하며 D를 학습한다. &lt;br /&gt;
A distributed bag of words: 문단에서 일정 길이의 context를 sample하여 문단 행렬 D로 이 문단의 단어들을 예측하는 학습을 한다.&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;Sentimental Analysis와 Information Retrieval 평가에서 Paragraph Vector가 압도적인 성능을 보였다. PV-DM이 PV-DBOW보다 더 나은 성능을 보이나 둘이 섞을 경우 최고의 성능을 보였다.&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 20:59:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/04/2.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/04/2.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Efficient Estimation of Word Representations in Vector Space</title>
        <description>&lt;h2 id=&quot;why&quot;&gt;WHY?&lt;/h2&gt;
&lt;p&gt;Continuous Word Representation을 평가할 수 있는 방법을 제시하고&lt;br /&gt;
더 많은 데이터들로 부터 더 빠르게 더 나은 표현을 학습할 수 있는 Continous Bag-of-Words(CBOW)와 Continous Skip-gram Model을 제시&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT?&lt;/h2&gt;
&lt;p&gt;NNLM : 앞 단어들로 뒷단어를 예측하도록 학습&lt;br /&gt;
CBOW : 앞 뒤의 단어들 각각에 대하여 중심 단어를 예측하도록 학습&lt;br /&gt;
Skip-gram : 중심 단어들로 앞 뒤의 단어들을 예측하도록 학습&lt;br /&gt;
실험: 여러 의미 관계의 Biggest - big + small 과 같은 문제를 얼마나 잘 학습했는지 확인&lt;/p&gt;

&lt;h2 id=&quot;so&quot;&gt;So&lt;/h2&gt;
&lt;p&gt;기존 방법들보다 훨씬 빠를 뿐만이 아니라 성능도 훨씬 좋았다. Skip-gram이 semantic의미를 가장 잘 확인했고 CBOW는 syntatic의미 파악에 정확도가 높았다.&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 20:08:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/nlp/2018/01/04/1.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/nlp/2018/01/04/1.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>신호와 소음</title>
        <description>&lt;p&gt;I. 예측에 대한 근본적인 의문들&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;경제: 경제 붕괴, 왜 전문가들은 예상하지 못했는가&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;정치: 내가 선거결과를 맞힌 비법&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;야구: 야구 경기는 왜 모든 ‘예측’의 모델이 되는가&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;II. 움직이는 과녁을 맞혀라!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;기상: 예측의 진보, 허리케인과 카오스의 원뿔&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;지진: 라퀼라의 재앙을 아무도 예상하지 못하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;평균과 불확실성:숫자에 속지 마라&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;전염병: 신종플루부터 에이즈까지&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;III.미래를 내 손에 움켜쥐는 법&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;베이즈 정리: 이기는 도바꾼은 어떻게 베팅하는가&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;체스: 컴퓨터가 인간처럼 미래를 내다볼 수 있을까&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;포커: 상대방의 허풍을 간파하는 법&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;IV. 보이지 않는 손이 세상을 움직인다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;주식: 개인은 절대 시장을 이길 수 없을까&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;지구 온난화: 얄팍한 선동인가 과학적 진리인가&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;테러: 진주만 공습과 9.11 테러의 공통점&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;우리가 아는 것과, 안다고 생각하는 것을 확실히 구분하고 이 차이를 줄여라.&lt;/p&gt;

&lt;p&gt;베이즈주의를 통해서.&lt;/p&gt;

&lt;p&gt;대체로 많은 사람들의 추측의 평균은, 개인의 평균보다 낫다.&lt;/p&gt;

&lt;p&gt;내가 가진 편견을 인정하고 점검해야 한다.&lt;/p&gt;

&lt;p&gt;편향을 없애야한다.&lt;/p&gt;

&lt;p&gt;예측할 수 없는 것에 대한 겸손함과 예측할 수 있는 것을 예측하는 용기, 그리고 이 둘 사이의 차이를 아는 지혜가 필요하다.&lt;/p&gt;

&lt;p&gt;불확실성을 적극 받아들여야 한다.&lt;/p&gt;

</description>
        <pubDate>Tue, 28 Feb 2017 23:45:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/rv/2017/02/28/%E1%84%89%E1%85%B5%E1%86%AB%E1%84%92%E1%85%A9%E1%84%8B%E1%85%AA-%E1%84%89%E1%85%A9%E1%84%8B%E1%85%B3%E1%86%B7.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/rv/2017/02/28/%E1%84%89%E1%85%B5%E1%86%AB%E1%84%92%E1%85%A9%E1%84%8B%E1%85%AA-%E1%84%89%E1%85%A9%E1%84%8B%E1%85%B3%E1%86%B7.html</guid>
        
        
        <category>RV</category>
        
      </item>
    
      <item>
        <title>휘게 라이프</title>
        <description>&lt;p&gt;1장 돈으로 살 수 없는 행복, 휘게&lt;/p&gt;

&lt;p&gt;2장 우리 모두를 위한 새로운 라이프스타일, ‘휘게’&lt;/p&gt;

&lt;p&gt;3장 ‘휘게’가 말하는 ‘함께’의 가치&lt;/p&gt;

&lt;p&gt;4장 누구나 덴마크 사람처럼 휘게를 즐길 수 있다&lt;/p&gt;

&lt;p&gt;5장 언제 어디서나 휘게 라이프&lt;/p&gt;

&lt;p&gt;6장 휘게가 머무는 식탁&lt;/p&gt;

&lt;p&gt;휘게 10계명&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;분위기: 조명을 조금 어둡게 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;지금 이 순간: 현재에 충실한다. 휴대전화를 끈다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;달콤한 음식: 커피, 초콜릿, 쿠키, 케이크, 사탕. 더 주세요!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;평등: ‘나’보다는 ‘우리’. 뭔가를 함께하거나 TV를 함께 시청한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;감사: 만끽하라. 오늘이 인생 최고의 날일지도 모른다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;조화: 우리는 경쟁을 하고 있는 것이 아니다. 우리는 이미 당신을 좋아한다. 당신이 무엇을 성취했든 뽐낼 필요가 없다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;편안함: 편안함을 느낀다. 휴식을 취한다. 긴장을 풀고 쉬는 것이 가장 중요하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;휴전: 감정 소모는 그만. 정치에 관해서라면 나중에 얘기한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;화목: 추억에 대해 이야기를 나눔으로써 관계를 다져보자. “기억나? 우리 저번에…”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;보금자리: 이곳은 당신의 세계다. 평화롭고 안전한 장소다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;집: 휘게크로그, 벽난로, 양초, 나무로 만들어진 것, 자연, 책, 도자기, 촉감, 빈티지, 담요와 쿠션&lt;/p&gt;

&lt;p&gt;양초, 양질의 초콜릿, 좋아하는 차, 좋아하는 책, 좋아하는 영화나 드라마, 잼, 모직양말, 편지, 스웨터, 일기장, 좋은 담요, 종이와 펜, 음악, 사진첩&lt;/p&gt;

&lt;p&gt;덴마크 램프: Ph램프, 클린트, 팬톤 VP글로브&lt;/p&gt;

</description>
        <pubDate>Mon, 13 Feb 2017 02:53:59 +0900</pubDate>
        <link>http://lyusungwon.github.io/rv/2017/02/12/%E1%84%92%E1%85%B1%E1%84%80%E1%85%A6-%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%91%E1%85%B3.html</link>
        <guid isPermaLink="true">http://lyusungwon.github.io/rv/2017/02/12/%E1%84%92%E1%85%B1%E1%84%80%E1%85%A6-%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%91%E1%85%B3.html</guid>
        
        
        <category>RV</category>
        
      </item>
    
  </channel>
</rss>
